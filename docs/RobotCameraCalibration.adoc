= AMR-ROS2 Rob2Cam Calibration - Developer Guide
:site-section: DeveloperGuide
:toc:
:toclevels: 3
:toc-title: Table of Contents
:toc-placement: preamble
:icons: font
:sectnums:
:imagesDir: dg-images
:librariesDir: ../libraries
:stylesDir: stylesheets
:xrefstyle: full
:experimental:
:linkattrs:
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:warning-caption: :warning:
endif::[]

:url-repo: https://github.com/OmronAPAC/Omron_AMR_ROS2
:url-ug: https://github.com/OmronAPAC/Omron_AMR_ROS2/blob/master/docs/RobotCameraCalibration.adoc

Last updated: `26 November 2021` By: `Zeon`

== Authors

* Zeon Chua Feiyi (link:https://github.com/CFZeon[CFZeon])

== Getting Started
=== Description
This package streamlines the collection of odometry data from the robot and Pose from the  ArUco marker before publishing it to the handeye_4dof_ros2 package and then writes the incoming calibrated input into a .yaml config file found in the pcl_processing package.

=== Hand-eye Calibration
image::handeye_diagram.png[Hand-eye Calibration]

Hand-eye calibration is an algorithm to calculate the transform between a robot arm and a camera mounted to the robot arm. This is represented as X in the diagram above. Using a fixed fiducial marker as denoted by M1, M2 and robot positional coordinates denoted by R1, R2, the transforms A and B can be obtained through vector resolution. 

Combining these gives us the equation AX=XB, which is the hand-eye calibration equation.

=== Rob2Cam Calibration
image::rob2cam_diagram.png[]

The Rob2Cam calibration calculates the Pose between the AMR's center point and the camera. This is done using an algorithm called hand-eye calibration, which as its name implies, calculates the distance between a robot's hand and its camera. Since we are not using a robot arm, the term hand-eye has been changed to Rob2Cam to remove any confusion.

Based on the diagram above, we are calculating AMR H Camera, given that we can provide multiple (>=3) unique Origin H AMR and Camera H Marker.

The AMR has only 3 degrees of freedom(DoF), which are x, y and theta(rotation around the Z-axis). An articulated robot arm has 6. Conventional hand-eye calibration algorithms are made under the assumption that the robot is of 6 DoF. This means that conventional hand-eye calibration algorithms are not able to calculate the Rob2Cam calibration that we require here.

As such, we use Ulrich's algorithm which is made to calculate hand-eye calibration for SCARA robots, which are 4DoF. Due to the lack of dimensions in movement, the algorithm is unable to calculate the Z-axis of the camera relative to the robot. The current solution is to directly measure it with a tool like a measuring tape.

[[prerequisites]]
=== Prerequisites

. **Omron_AMR_ROS2**
+
These sets of packages assume that you already have the Omron_AMR_ROS2 packages set up.
+
https://github.com/OmronAPAC/Omron_AMR_ROS2/blob/master/docs/DeveloperGuide.adoc[Omron_AMR_ROS2 Developer's Guide]


. **Handeye 4dof**
+
You will also need the Handeye-4dof calibration algorithm from https://github.com/QuantuMope/handeye-4dof[here]. The ROS2 implementation is included in this package. Look link:https://github.com/CFZeon/handeye_4dof_ros2[here] for instructions on how to operate it.

. **ros2_aruco**
+
The ROS2 ArUco package uses OpenCV to obtain the pose of the ArUco markers relative to the camera. You can find it link:https://github.com/JMU-ROBOTICS-VIVA/ros2_aruco[here].

== Software Design
[[architecture]]
=== Architecture
An overview of this package architecture and its dependencies is summarised in the diagram below:

image::rob2cam_architecture.png[]

=== Calibration Node

==== ArUco marker Pose Subscriber
The marker pose subscriber constantly listens for new incoming ArUco marker data.

==== AMR Status Subscriber
The status subscriber constantly listens for new incoming data from the AMR including its odometry data.

==== Store Poses Service
The store_poses service appends the current pose of the AruCo marker and the AMR to 2 separate lists when this empty service is called. 

As the AMR only provides information in x, y, theta, a conversion is done to convert it to a Pose before it is published to the handeye_4dof_ros2 node. This can be found in the quat_from_yaw() function.

On a ZED2 stereo camera, it is necessary to rotate the camera to marker orientation by -1.57, 0, -1.57 as the camera faces X forward, as opposed to Z forward. The pose should be transformed in the marker callback before it is appended into the list.

==== calibrated_pose Subscriber Callback

Upon receiving a valid calculation from the handeye_4dof_ros2 node, the calibration result is saved into the .yaml file in pcl_processing.

The file is read and split into its individual lines with 
`file.readlines()`

Within each line, the specific identifier i.e. `camera_roll_x_offset` is searched for and the value is then replaced by the calibrated value.

The modified lines are then written back into the file.

handeye_4dof algorithm does not calculate Z-axis offset. This is excluded from the file writing process.

== Instructions on Usage
[[instructions]]
. Switch to the obstacle avoidance branch.
+
....
git switch obs_avoidance_devel
....
. Build the packages
+
....
colcon build --symlink-install
....
. Source the built packages
+
....
source install/setup.bash
....
. Ensure your host machine is running the ARCL action server. It can be launched with
+
....
ros2 launch om_aiv_util server.launch.py
....
. Run `rob2cam_calibration` package with
+
....
ros2 run rob2cam_calibration calibration_node
....
. Ensure that your camera feed is streaming properly. We used the ZED2. Instructions on running the ZED2 can be found link:https://github.com/stereolabs/zed-ros-wrapper[here]. Launch the Zed2 ROS2 nodes with
+
....
ros2 launch zed_wrapper zed2.launch.py
....
. Run the ArUco Marker Node with
+
....
ros2 run ros2_aruco aruco_node
....
. Ensure that the marker is being properly detected with
+
....
ros2 topic echo aruco_poses
....
. Finally, run the handeye-4dof node using 
+
....
ros2 run handeye_4dof_ros2 handeye_4dof_node
....
. Open a new console and type rqt.
. In the rqt interface, select store_poses service.
+
image::rqt_store_poses.png[]
. Drive the robot to a location where the camera can see the aruco marker.
. Click on the Call button in rqt.
. Repeat steps 12 and 13 until a good sample size is obtained.
. Navigate to calculate_calibration service in rqt.
. Click on the Call button in rqt.
